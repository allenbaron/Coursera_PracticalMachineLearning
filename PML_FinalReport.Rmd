---
title: "Predicting Curling Activity with Machine Learning"
subtitle: "Final Project - Practical Machine Learning, Couersera"
output: html_notebook
---

```{r libraries, include = FALSE}
library(tidyverse)
library(caret)
library(doParallel)
```

# Executive Summary
This report summarizes the development of algorithms to predict with high accuracy the type of weightlift curl (`classe`) completed by participants using data provided by _Velloso, E., et al_. Two key goals are achieved in this report by differing algorithms:

- **Goal 1:** Predict the curl `classe` correctly for all 20 observations on the final quiz for the **Coursera: Practical Machine Learning** course
    - **Model:** Random forest trained solely on "metafeatures" (_i.e._ features in the data the describe how the data was collected: `user_name` and time-related features).
- **Goal 2:** Maximize prediction accuracy of the curl `classe` using data that _could_ be generated in a _quasi_real-life scenario.
    - **Model 1:** Random forest trained on all raw data
    - **Model 2:** Random forest trained on _ONLY_ data collected from sensors on the forearm (because forearm sensors in activity trackers and smart watches are fairly common, while other sensors would need

The primary goal, to predict the curl type of 20 observations as part of the final quiz for the Coursera: Practical Machine Learning course, is achieved using a random forest (accuracy $\geq$ 99%) trained using only the name of participants and time-based variables. A secondary and more "real-life" goal, to predict the curl type accurately using variables measured by various sensors, is achieved with ...

- include final algorithm(s) & essential attributes: accuracy, features, etc.

# Introduction
_Velloso, E., et al_ collected accelerometer, gyrometer, and magnetometer data from each location detailed in the image below for 5 types of unilateral dumbbell weightlift curl (A-E), where A was the correct procedure and B-E represented specific errors: B = moving elbow forward, C = lifting only halfway, D = lowering only halfway, and E = moving hips forward. For more details, see _Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/har#ixzz57fKGqFGo). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013._

![](on-body-sensing-schema.png){ width=20% }

Here, the data of _Velloso, E., et al_ is used to develop algorithms to predict which type of bicep curl, with 2 main goals:

1. To accurately predict the curl type of 20 observations selected randomly as a final quiz for the Coursera: Practical Machine Learning course.
    - To correctly type all 20 observations with 95% confidence the model accuracy must be at least $1 - \frac{0.05}{20}$ = `r round(1- 0.05/20, 4)` (using a bonferroni correction). This assumes independence among the 20 observations, which is unlikely, but it makes a rough estimation possible.
2. To create an algorithm that _could_ be used in a production environment to provide real-time feedback to weight lifters about the quality of their bicep curl. Although the average person would likely not have access to so many sensors, there is no attempt to limit which sensor data the model is trained on.

# Data Import and Formatting
```{r import_data, results = 'hold', message = FALSE, warning = FALSE}
weightlift <- readr::read_csv(
    "pml-training.csv",
    na = c("", "NA", "#DIV/0!"),
    col_types = cols(
        .default = col_double(),
        cvtd_timestamp = col_date("%d/%m/%Y %H:%M"),
        user_name = col_factor(
            levels = c("carlitos", "pedro", "adelmo", "charles", "eurico", "jeremy")
            ),
        new_window = col_factor(levels = c("no", "yes")),
        classe = col_factor(levels = c("A", "B", "C", "D", "E"))
        )
    )

readr::problems(weightlift)
```

```{r split_for_oob}
inTrain <- caret::createDataPartition(weightlift$X1, p = 0.7, list = FALSE)
wl_train <- weightlift[inTrain, ]
wl_test <- weightlift[-inTrain, ]
```


# Model 1 - Training on User + Time

participants can't do all 5 activities at same time, can likely predict well by `user_name` and time-related variables only.

describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did

```{r time_plot}
wl_train <- dplyr::mutate(
    wl_train,
    timestamp = as.numeric(
        paste(raw_timestamp_part_1, raw_timestamp_part_2, sep = "." )
        )
    )
    
ggplot2::ggplot(data = wl_train, aes(x = classe, y = timestamp)) +
    geom_boxplot() +
    facet_wrap(~user_name, scales = "free")
```

```{r doParallel}
cl <- parallel::makeCluster(parallel::detectCores() - 1)
doParallel::registerDoParallel(cl)
```

see Max Kuhn's analysis
```{r}
fitControl <- trainControl(method = "cv",
                           number = 10,
                           allowParallel = TRUE)                         
```

```{r time_rf}
time_rf <- train(classe ~ user_name + timestamp,method = "rf", data = wl_train,
                       trControl = fitControl
                       )
time_rf
```

```{r time_rf_all, include = FALSE}
time_rf_all <- train(classe ~ user_name + timestamp + num_window + new_window + cvtd_timestamp, method = "rf", data = wl_train,
                       trControl = fitControl
                       )
time_rf_all
```

```{r load_rf_importance, include = FALSE}
rf_importance <- function(importance) {
    importance %>%
        as.data.frame() %>%
        tibble::rownames_to_column(var = "feature") %>%
        dplyr::arrange(desc(MeanDecreaseGini)) %>%
        mutate(MeanDecreaseGini = round(MeanDecreaseGini, 2))
}
```

```{r time_importance}
rf_importance(time_rf_all$finalModel$importance)
```

```{r time_rf3, include = FALSE}
time_rf3 <- train(classe ~ user_name + timestamp + num_window,
                   method = "rf",
                   data = wl_train,
                   trControl = fitControl
                   )
time_rf3
```

```{r DEregisterCluster}
stopCluster(cl)
registerDoSEQ()
```

```{r prep_test}
wl_test <- dplyr::mutate(
    wl_test,
    timestamp = as.numeric(
        paste(raw_timestamp_part_1, raw_timestamp_part_2, sep = "." )
        )
    )
```

```{r load_oob_accuracy, include = FALSE}
oob_accuracy <- function(test_set, outcome_var, ..., accuracy_only = TRUE) {
    # ... = models from caret::train()
    # accuracy_only = if FALSE, returns accuracy and data.frame of raw predictions
    
    library(purrr)

    model_names <- map_chr(as.list(substitute(list(...)))[-1L], deparse)
    model <- list(...)
    
    outcome <- eval(substitute(outcome_var), test_set, parent.frame())
    pred_list <- map(eval(model), .f = predict, newdata = test_set)
    accuracy <- map_dbl(pred_list, ~sum(. == outcome)/length(outcome))
    names(accuracy) <- model_names
    
    if (accuracy_only) {
        return(accuracy)
    } else {
        names(pred_list) <- model_names
        return(list(accuracy = accuracy, prediction = as.data.frame(pred_list)))
    }
}
```

```{r}
oob_accuracy(wl_test, classe, time_rf, time_rf_all, time_rf3)
```

```{r import_quiz, warning = FALSE}
quiz20 <- readr::read_csv("pml-testing.csv", na = c("", "NA", "#DIV/0!"),
                           col_types = spec(weightlift) # for consistent loading
                           )
readr::problems(quiz20)

quiz20 <- dplyr::mutate(
    quiz20,
    timestamp = as.numeric(
        paste(raw_timestamp_part_1, raw_timestamp_part_2, sep = "." )
        )
    )
```

```{r quiz_prediction, results = 'hide'}
quiz_prediction <- predict(time_rf3, quiz20)
quiz_prediction
```

```{r quiz_actual, include = FALSE}
quiz_actual <- c('B', 'A', 'B', 'A', 'A', 'E', 'D', 'B', 'A', 'A',
                 'B', 'C', 'B', 'A', 'E', 'E', 'A', 'B', 'B', 'B')

all(quiz_prediction == quiz_actual)
```

# Model 2 - ????

```{r load_examine_NA, include = FALSE}
# Return total (& percent) NA's in data frame and plot of percent NA's by column

examine_NA <- function(df, x_lim = NULL, bw = NULL, plot = TRUE, ...) {
    # df = data.frame
    # x_lim = ggplot2::histogram() xlim value, if desired
    # bw = ggplot2::histogram() bw value, if desired
    # plot = logical; whether or not to print plot of percent NA vals by column
    # ... = other arguments passed to ggplot2::qplot()
    
    df_NA <- purrr::map_dfc(df, is.na)
    ttl_NA <- sum(as.matrix(df_NA))
    ttl_obs <- prod(dim(df)) #OR length(as.matrix(df))
    
    if(ttl_NA == 0) {
        return("No NA values in data set")
    }
    # print total & proportion NA
    print(
        paste(
            ttl_NA, "NA values of", ttl_obs, "observations =",
            round(ttl_NA/ttl_obs*100, 2), "percent."
        )
    )
    
    # create plot
    if (plot == TRUE) {
    NA_by_col <- purrr::map_dbl(df_NA, ~round(sum(.)/length(.)*100, 2))
    rng <- range(NA_by_col)
    
    if (is.null(x_lim)) {
        if (diff(rng) < 10) {
            lower <- ((mean(rng) - 5) + abs(mean(rng) - 5)) / 2
            x_lim <- c(lower, lower + 10)
        } else {
            x_lim <- rng
        }
    }
    if (is.null(bw)) {
        bw <- diff(x_lim)/25
    }
    

        print(
            ggplot2::qplot(NA_by_col, xlab = "Percent NA", binwidth = bw, ...) +
                ggplot2::coord_cartesian(xlim = x_lim)
        )
    }
    
    # return useful variables invisibly
    invisible(
        list(
            df_NA_lgl = df_NA, # TRUE if position in df is NA
            df_NA_n = ttl_NA,
            col_NA_n = purrr::map_int(df_NA, sum), # count of NA per column
            col_NA_pct = NA_by_col # percent of NA per column
        )
    )
    
}
```

Custom function returns total # of NA values in data frame and plots % NA by variable
```{r missing, fig.width = 4, fig.height = 3}
wl_train_NA <- examine_NA(df = wl_train, ylab = "Number of Columns")
```

Number of actual values (_i.e._ not NA) in each column that's missing most data.
```{r vals_in_hi_NA_cols}
summary(nrow(wl_train) - wl_train_NA$col_NA_n[wl_train_NA$col_NA_pct > 50])
```

remove time and high NA columns (probably from sliding window computations but imputation too difficult, probably not enough calculated data to train model with sufficient accuracy for quiz)

```{r rmv_NA_vars}
keep_vars <- wl_train_NA$col_NA_pct < 50 &
                !grepl("time|window|X1|user", names(wl_train))

wl_sensor <- dplyr::select(weightlift, names(keep_vars)[keep_vars])

snsr_train <- wl_sensor[inTrain, ]
snsr_test <- wl_sensor[-inTrain, ]
```

```{r doParallel2, include = FALSE}
cl <- parallel::makeCluster(parallel::detectCores() - 1)
doParallel::registerDoParallel(cl)
```

```{r multinomial_log_regression, include = FALSE}
snsr_mlr <- train(classe ~ ., method = "multinom", data = snsr_train,
                   trControl = fitControl,
                   trace = FALSE)
snsr_mlr
```

```{r lda, include = FALSE}
snsr_lda <- train(classe ~ ., method = "lda", data = snsr_train,
                   trControl = fitControl)
snsr_lda
```

```{r steplda, include = FALSE}
snsr_slda <- train(classe ~ ., method = "stepLDA", data = snsr_train,
                    trControl = fitControl)

# defaults see [stepclass()](https://www.rdocumentation.org/packages/klaR/versions/0.6-12/topics/stepclass)
#   - maxvar = Inf
#   - direction = "both"
#   - criterion = "CR" 
snsr_slda
```

```{r best_vars, include = FALSE}
# trying to determine which vars were best for each model
#   Only figured out stepwise LDA
snsr_mlr$finalModel
snsr_lda$finalModel
snsr_slda$finalModel$model # used only 2 vars but stopped at 40% correctness rate
```

```{r snsr_oob, include = FALSE}
oob_accuracy(snsr_test, classe, snsr_mlr, snsr_lda, snsr_slda)
```

```{r DEregisterCluster2, include = FALSE}
stopCluster(cl)
registerDoSEQ()
```


Use `parRF` = random forest with automatic feature selection
```{r doParallel3, include = FALSE}
cl <- parallel::makeCluster(parallel::detectCores() - 1)
doParallel::registerDoParallel(cl)
```

```{r snsr_rf}
snsr_parRF <- train(classe ~ ., method = "parRF",
                    data = snsr_train,
                    trControl = fitControl
                    )
snsr_parRF
```

```{r DEregisterCluster3, include = FALSE}
stopCluster(cl)
registerDoSEQ()
```


# Model Training & Evaluation
describing how you built your model, how you used cross validation, what you think the expected out of sample error is, and why you made the choices you did

```{r snsr_mdl}
oob_accuracy(snsr_test, classe, snsr_parRF)

rf_importance(snsr_parRF$finalModel$importance)
```

```{r train_forearm}
# train on forearm info only

# get data (no NA, forearm only)
forearm_keep <- wl_train_NA$col_NA_pct < 50 &
    grepl("forearm", names(wl_train))
wl_forearm <- dplyr::select(weightlift, classe, names(forearm_keep)[forearm_keep])

# split data
forearm_train <- wl_forearm[inTrain, ]
forearm_test <- wl_forearm[-inTrain, ]
```

```{r doParallel_forearm, include = FALSE}
cl <- parallel::makeCluster(parallel::detectCores() - 1)
doParallel::registerDoParallel(cl)
```

```{r forearm_rf}
forearm_parRF <- train(classe ~ ., method = "parRF",
                    data = forearm_train,
                    trControl = fitControl
                    )
forearm_parRF
```

```{r DEregisterCluster_forearm, include = FALSE}
stopCluster(cl)
registerDoSEQ()
```

```{r forearm_results}
oob_accuracy(forearm_test, classe, forearm_parRF)

rf_importance(forearm_parRF$finalModel$importance)
```

# Appendix (for Reproducibility)

## `examine_NA`
```{r show_examine_NA, eval = FALSE}
# Return total (& percent) NA's in data frame and plot of percent NA's by column

examine_NA <- function(df, x_lim = NULL, bw = NULL, plot = TRUE, ...) {
    # df = data.frame
    # x_lim = ggplot2::histogram() xlim value, if desired
    # bw = ggplot2::histogram() bw value, if desired
    # plot = logical; whether or not to print plot of percent NA vals by column
    # ... = other arguments passed to ggplot2::qplot()
    
    df_NA <- purrr::map_dfc(df, is.na)
    ttl_NA <- sum(as.matrix(df_NA))
    ttl_obs <- prod(dim(df)) #OR length(as.matrix(df))
    
    if(ttl_NA == 0) {
        return("No NA values in data set")
    }
    # print total & proportion NA
    print(
        paste(
            ttl_NA, "NA values of", ttl_obs, "observations =",
            round(ttl_NA/ttl_obs*100, 2), "percent."
        )
    )
    
    # create plot
    if (plot == TRUE) {
    NA_by_col <- purrr::map_dbl(df_NA, ~round(sum(.)/length(.)*100, 2))
    rng <- range(NA_by_col)
    
    if (is.null(x_lim)) {
        if (diff(rng) < 10) {
            lower <- ((mean(rng) - 5) + abs(mean(rng) - 5)) / 2
            x_lim <- c(lower, lower + 10)
        } else {
            x_lim <- rng
        }
    }
    if (is.null(bw)) {
        bw <- diff(x_lim)/25
    }
    

        print(
            ggplot2::qplot(NA_by_col, xlab = "Percent NA", binwidth = bw, ...) +
                ggplot2::coord_cartesian(xlim = x_lim)
        )
    }
    
    # return useful variables invisibly
    invisible(
        list(
            df_NA_lgl = df_NA, # TRUE if position in df is NA
            df_NA_n = ttl_NA,
            col_NA_n = purrr::map_int(df_NA, sum), # count of NA per column
            col_NA_pct = NA_by_col # percent of NA per column
        )
    )
    
}
```

## `rf_importance`
```{r show_rf_importance, eval = FALSE}
rf_importance <- function(importance) {
    importance %>%
        as.data.frame() %>%
        tibble::rownames_to_column(var = "feature") %>%
        dplyr::arrange(desc(MeanDecreaseGini)) %>%
        dplyr::mutate(MeanDecreaseGini = round(MeanDecreaseGini, 2))
}
```

## `oob_accuracy`
```{r show_oob_accuracy, include = FALSE}
oob_accuracy <- function(test_set, outcome_var, ..., accuracy_only = TRUE) {
    # ... = models from caret::train()
    # accuracy_only = if FALSE, returns accuracy and data.frame of raw predictions
    
    library(purrr)

    model_names <- map_chr(as.list(substitute(list(...)))[-1L], deparse)
    model <- list(...)
    
    outcome <- eval(substitute(outcome_var), test_set, parent.frame())
    pred_list <- map(eval(model), .f = predict, newdata = test_set)
    accuracy <- map_dbl(pred_list, ~sum(. == outcome)/length(outcome))
    names(accuracy) <- model_names
    
    if (accuracy_only) {
        return(accuracy)
    } else {
        names(pred_list) <- model_names
        return(list(accuracy = accuracy, prediction = as.data.frame(pred_list)))
    }
}
```