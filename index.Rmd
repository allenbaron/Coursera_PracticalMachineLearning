---
title: "Final Project - Practical Machine Learning, Coursera"
author: "Allen Baron"
subtitle: Predicting Curling Activity with Machine Learning
output:
  html_notebook:
    toc: yes
    toc_float: yes
  html_document:
    df_print: paged
    toc: yes
---

```{r libraries, include = FALSE}
library(tidyverse)
library(caret)
library(doParallel)
```

# Introduction
My primary goal with this project is to predict the type of weightlift curl (`classe` variable) with sufficient accuracy to correctly predict all 20 observations on the final quiz for the **Coursera: Practical Machine Learning** course. The data comes from the work of _[Velloso, E., et al.](#cite)_ and includes accelerometer, gyrometer, and magnetometer measurements at each of the locations detailed in the image below for 5 different types of weightlift curl, where the correct procedure is labelled `A` and 4 other incorrect procedures are `B`-`E`.

<div style="text-align:center" markdown="1">
![**Sensor Locations**](on-body-sensing-schema.png){ width=20% }
</div>

To predict all 20 observations with 95% confidence, I estimate the final model will need to have an accuracy of $1 - \frac{0.05}{20}$ = `r round(1- 0.05/20, 4)` ([bonferroni correction](https://en.wikipedia.org/wiki/Family-wise_error_rate#Controlling_procedures)). This essentially means the final out-of-sample error rate must be less than `r round(0.05/20, 4)` ($1 - accuracy$).

After importing the training data without errors, I split the data into training and testing sets (to later provide out-of-sample error estimates).

```{r import_data, results = 'hide', message = FALSE, warning = FALSE}
weightlift <- readr::read_csv(
    "pml-training.csv",
    na = c("", "NA", "#DIV/0!"),
    col_types = cols(
        .default = col_double(),
        cvtd_timestamp = col_datetime("%d/%m/%Y %H:%M"),
        user_name = col_factor(
            levels = c("carlitos", "pedro", "adelmo", "charles", "eurico", "jeremy")
            ),
        new_window = col_factor(levels = c("no", "yes")),
        classe = col_factor(levels = c("A", "B", "C", "D", "E"))
        )
    )

readr::problems(weightlift) # import errors - none present

inTrain <- caret::createDataPartition(weightlift$X1, p = 0.7, list = FALSE)
wl_train <- weightlift[inTrain, ]
wl_test <- weightlift[-inTrain, ]
```

This data set has what I call "metafeatures" (_i.e._ features that describe how the data was collected, such as `user_name`, `num_window`, and `cvtd_timestamp`). Because any individual participant can only complete one activity at a time, it seems reasonable to guess that a model trained solely on metafeatures will have very high accuracy (for the final quiz predictions).

For curiosity's sake, I'll train two different models: the **[first](#model1)** on metafeatures, and the **[second](#model2)** strictly with sensor measurements, which is what I would expect to use in a real world application.

# Cross validation & Out-of-sample error procedures
For all of the models I use 10-fold cross-validation for training, based on the recommendation of [Max Kuhn](http://appliedpredictivemodeling.com/blog/2014/11/27/vpuig01pqbklmi72b8lcl3ij5hj2qm), and then estimate out-of-sample accuracy/error using the 30% of the data set aside for testing earlier.

# Model 1 - Training on Metafeatures {#model1}
Combining `user_name` with at least one time-based variable will likely be enough to predict `classe` with high accuracy. The `cvtd_timestamp` values are probably too homogeneous (there are only `r wl_train$cvtd_timestamp %>% unique() %>% length()` unique values) but the raw timestamp features combined (now `timestamp`) provide a unique value for each observation and appear to provide sufficient separation to build a model.

```{r time_plot}
wl_train <- dplyr::mutate(
    wl_train,
    timestamp = as.numeric(
        paste(raw_timestamp_part_1, raw_timestamp_part_2, sep = "." )
        )
    )
    
ggplot2::ggplot(data = wl_train, aes(x = classe, y = timestamp)) +
    geom_boxplot() +
    facet_wrap(~user_name, scales = "free") +
    ggtitle("Separation of `classe` type by user and timestamp", subtitle = NULL)
```

I will train a random forest algorithm because they generally perform well on classification problems out of the box.

```{r time_rf}
# Register cluster for parallel computation
cl <- parallel::makeCluster(parallel::detectCores() - 1)
doParallel::registerDoParallel(cl)

# set 10-fold CV
fitControl <- trainControl(method = "cv",
                           number = 10,
                           allowParallel = TRUE)                         
# train model
time_rf <- train(classe ~ user_name + timestamp, method = "rf", data = wl_train,
                       trControl = fitControl
                       )
time_rf
```

The resulting model is extremely accurate but doesn't quite reach the desired accuracy threshold. I suspect including 1 or more additional "metafeatures" would put the model over that threshold. Among those, `num_window` has the most unique values so I'll use it.

```{r most_unique_vals}
wl_train %>%
    select(contains("cvtd"), contains("window")) %>%
    purrr::map_int(~length(unique(.)))
```

```{r time_rf3}
time_rf3 <- train(classe ~ user_name + timestamp + num_window,
                   method = "rf",
                   data = wl_train,
                   trControl = fitControl
                   )

# Deregister cluster
stopCluster(cl)
registerDoSEQ()
```

### Final Metafeature Model Accuracy and Error {#rf3}
The retrained model approaches perfect classification on both the training and testing set. It also performed flawlessly on the quiz (not shown).
```{r time_rf3_eval, results = 'asis'}
# add timestamp feature to testing set
wl_test <- dplyr::mutate(
    wl_test,
    timestamp = as.numeric(
        paste(raw_timestamp_part_1, raw_timestamp_part_2, sep = "." )
        )
    )

# training set accuracy
train_acc <- max(time_rf3$results$Accuracy)

# out-of-sample accuracy/error for testing set
test_acc <- predict(time_rf3, newdata = wl_test) %>%
    {sum(. == wl_test$classe) / nrow(wl_test)}

knitr::kable(tibble(
    dataset = c("training", "testing"),
    accuracy = c(train_acc, test_acc),
    error = 1 - accuracy
    ))
```

```{r quiz, include = FALSE}
quiz_actual <- c("B", "A", "B", "A", "A", "E", "D", "B", "A", "A",
                 "B", "C", "B", "A", "E", "E", "A", "B", "B", "B")

quiz20 <- readr::read_csv("pml-testing.csv", na = c("", "NA", "#DIV/0!"),
    col_types = spec(weightlift)) # for loading consistency

quiz20 <- dplyr::mutate(
    quiz20,
    timestamp = as.numeric(
        paste(raw_timestamp_part_1, raw_timestamp_part_2, sep = "." )
        )
    )

sum(predict(time_rf3, quiz20) == quiz_actual)
```


# Model 2 - Training on Sensor Data {#model2}
The overall training data set has a large number of missing values which are concentrated  in about 100 of the features (see [`examine_NA`](#eNA) for code). 
```{r load_examine_NA, include = FALSE}
# Return total (& percent) NA's in data frame and plot of percent NA's by column

examine_NA <- function(df, x_lim = NULL, bw = NULL, plot = TRUE, ...) {
    # df = data.frame
    # x_lim = ggplot2::histogram() xlim value, if desired
    # bw = ggplot2::histogram() bw value, if desired
    # plot = logical; whether or not to print plot of percent NA vals by column
    # ... = other arguments passed to ggplot2::qplot()
    
    df_NA <- purrr::map_dfc(df, is.na)
    ttl_NA <- sum(as.matrix(df_NA))
    ttl_obs <- prod(dim(df)) #OR length(as.matrix(df))
    
    if(ttl_NA == 0) {
        return("No NA values in data set")
    }
    # print total & proportion NA
    print(
        paste(
            ttl_NA, "NA values of", ttl_obs, "observations =",
            round(ttl_NA/ttl_obs*100, 2), "percent."
        )
    )
    
    # create plot
    if (plot == TRUE) {
    NA_by_col <- purrr::map_dbl(df_NA, ~round(sum(.)/length(.)*100, 2))
    rng <- range(NA_by_col)
    
    if (is.null(x_lim)) {
        if (diff(rng) < 10) {
            lower <- ((mean(rng) - 5) + abs(mean(rng) - 5)) / 2
            x_lim <- c(lower, lower + 10)
        } else {
            x_lim <- rng
        }
    }
    if (is.null(bw)) {
        bw <- diff(x_lim)/25
    }
    

        print(
            ggplot2::qplot(NA_by_col, xlab = "Percent NA", binwidth = bw, ...) +
                ggplot2::coord_cartesian(xlim = x_lim)
        )
    }
    
    # return useful variables invisibly
    invisible(
        list(
            df_NA_lgl = df_NA, # TRUE if position in df is NA
            df_NA_n = ttl_NA,
            col_NA_n = purrr::map_int(df_NA, sum), # count of NA per column
            col_NA_pct = NA_by_col # percent of NA per column
        )
    )
    
}
```

```{r missing, fig.height = 3, fig.width = 4}
wl_train_NA <- examine_NA(df = wl_train, ylab = "Number of Columns")
```

While those 100 features may be useful for application in a real world context (a partial goal of this model), they will probably not be useful for predicting the 20 quiz observations. Also, missing values are problematic for machine learning algorithms and these are not easily imputed in a meaningful way (_the easiest approach might be duplication, but would it be helpful?_). The best approach, then, for training a real world model (that still works for the quiz) is to remove features with missing data along with the metafeatures.

```{r rmv_NA_vars}
keep_vars <- wl_train_NA$col_NA_pct < 50 &
                !grepl("time|window|X1|user", names(wl_train))

snsr_train <- dplyr::select(wl_train, names(keep_vars)[keep_vars])
```

That leaves the data set with 52 features to train on. I expect a random forest algorithm would again perform best here but I'll compare that against multinomial logistic regression, linear discriminant analysis (LDA), and step-wise LDA.
```{r doParallel2, include = FALSE}
cl <- parallel::makeCluster(parallel::detectCores() - 1)
doParallel::registerDoParallel(cl)
```

```{r real_models}
snsr_rf <- train(classe ~ ., method = "rf", data = snsr_train,
                    trControl = fitControl
                    )

snsr_mlr <- train(classe ~ ., method = "multinom", data = snsr_train,
                   trControl = fitControl,
                   trace = FALSE)

snsr_lda <- train(classe ~ ., method = "lda", data = snsr_train,
                   trControl = fitControl)

snsr_slda <- train(classe ~ ., method = "stepLDA", data = snsr_train,
                    trControl = fitControl)
```

```{r DEregisterCluster2, include = FALSE}
stopCluster(cl)
registerDoSEQ()
```

```{r load_oob_accuracy, include = FALSE}
oob_accuracy <- function(test_set, outcome_var, ..., accuracy_only = TRUE) {
    # ... = models from caret::train()
    # accuracy_only = if FALSE, returns accuracy and data.frame of raw predictions
    
    library(purrr)

    model_names <- map_chr(as.list(substitute(list(...)))[-1L], deparse)
    model <- list(...)
    
    outcome <- eval(substitute(outcome_var), test_set, parent.frame())
    pred_list <- map(eval(model), .f = predict, newdata = test_set)
    accuracy <- map_dbl(pred_list, ~sum(. == outcome)/length(outcome))
    names(accuracy) <- model_names
    
    if (accuracy_only) {
        return(accuracy)
    } else {
        names(pred_list) <- model_names
        return(list(accuracy = accuracy, prediction = as.data.frame(pred_list)))
    }
}
```

### Final Sensor Model Accuracies and Errors
Based on the out-of-sample accuracy of each model (supplied by [`oob_accuracy`](#oob)), the random forest algorithm definitely performed the best.
```{r snsr_oob, results = "asis"}
snsr_test <- dplyr::select(wl_test, names(keep_vars)[keep_vars])

knitr::kable(oob_accuracy(snsr_test, classe, snsr_rf, snsr_mlr, snsr_lda, snsr_slda) %>%
    data.frame(accuracy = .) %>%
    tibble::rownames_to_column(var = "model") %>%
    dplyr::mutate(error = 1 - accuracy) %>%
    dplyr::arrange(error))
```
I'm impressed that it achieved an accuracy that was close to that of the [3-metafeature random forest](#rf3). Like that model it correctly predicted `r sum(predict(snsr_rf, quiz20) == quiz_actual)` observations on the final quiz, while the other algorithms would not have obtained a passing grade (`snsr_mlr` = `r sum(predict(snsr_mlr, quiz20) == quiz_actual)`
 correct, `snsr_lda` = `r sum(predict(snsr_lda, quiz20) == quiz_actual)`, and `snsr_slda` = `r sum(predict(snsr_slda, quiz20) == quiz_actual)`).
 
# Appendix

## Cited work {#cite}
_Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/har#ixzz57fKGqFGo). Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13). Stuttgart, Germany: ACM SIGCHI, 2013._

## Custom R functions

### `examine_NA` {#eNA}
```{r show_examine_NA, eval = FALSE}
# Return total (& percent) NA's in data frame and plot of percent NA's by column

examine_NA <- function(df, x_lim = NULL, bw = NULL, plot = TRUE, ...) {
    # df = data.frame
    # x_lim = ggplot2::histogram() xlim value, if desired
    # bw = ggplot2::histogram() bw value, if desired
    # plot = logical; whether or not to print plot of percent NA vals by column
    # ... = other arguments passed to ggplot2::qplot()
    
    df_NA <- purrr::map_dfc(df, is.na)
    ttl_NA <- sum(as.matrix(df_NA))
    ttl_obs <- prod(dim(df)) #OR length(as.matrix(df))
    
    if(ttl_NA == 0) {
        return("No NA values in data set")
    }
    # print total & proportion NA
    message(
        paste(
            ttl_NA, "NA values of", ttl_obs, "observations =",
            round(ttl_NA/ttl_obs*100, 2), "percent."
        )
    )
    
    # create plot
    if (plot == TRUE) {
    NA_by_col <- purrr::map_dbl(df_NA, ~round(sum(.)/length(.)*100, 2))
    rng <- range(NA_by_col)
    
    if (is.null(x_lim)) {
        if (diff(rng) < 10) {
            lower <- ((mean(rng) - 5) + abs(mean(rng) - 5)) / 2
            x_lim <- c(lower, lower + 10)
        } else {
            x_lim <- rng
        }
    }
    if (is.null(bw)) {
        bw <- diff(x_lim)/25
    }
    

        print(
            ggplot2::qplot(NA_by_col, xlab = "Percent NA", binwidth = bw, ...) +
                ggplot2::coord_cartesian(xlim = x_lim)
        )
    }
    
    # return useful variables invisibly
    invisible(
        list(
            df_NA_lgl = df_NA, # TRUE if position in df is NA
            df_NA_n = ttl_NA,
            col_NA_n = purrr::map_int(df_NA, sum), # count of NA per column
            col_NA_pct = NA_by_col # percent of NA per column
        )
    )
    
}
```

### `oob_accuracy` {#oob}
```{r show_oob_accuracy, eval = FALSE}
oob_accuracy <- function(test_set, outcome_var, ..., accuracy_only = TRUE) {
    # ... = models from caret::train()
    # accuracy_only = if FALSE, returns accuracy and data.frame of raw predictions
    
    library(purrr)

    model_names <- map_chr(as.list(substitute(list(...)))[-1L], deparse)
    model <- list(...)
    
    outcome <- eval(substitute(outcome_var), test_set, parent.frame())
    pred_list <- map(eval(model), .f = predict, newdata = test_set)
    accuracy <- map_dbl(pred_list, ~sum(. == outcome)/length(outcome))
    names(accuracy) <- model_names
    
    if (accuracy_only) {
        return(accuracy)
    } else {
        names(pred_list) <- model_names
        return(list(accuracy = accuracy, prediction = as.data.frame(pred_list)))
    }
}
```

## For Reproducibility
```{r}
sessionInfo()
```

